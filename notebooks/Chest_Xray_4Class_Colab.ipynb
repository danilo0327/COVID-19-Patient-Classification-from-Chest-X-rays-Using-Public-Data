{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c8d2c29c",
   "metadata": {},
   "source": [
    "# Chest X‑ray 4‑Class Classifier (Colab, PyTorch, T4 GPU)\n",
    "\n",
    "This notebook trains a classifier for **COVID**, **Lung_Opacity**, **Normal**, and **Viral Pneumonia** using **PyTorch** on Google Colab (T4 GPU).  \n",
    "It supports:\n",
    "- Reading a `dataset.csv` which contains the paths to images and their labels.\n",
    "- Robust path resolution (handles Windows paths or relative paths).\n",
    "- Class imbalance handling (weighted sampler).\n",
    "- Two training tracks:\n",
    "  - **A. Small CNN baseline** (from scratch)\n",
    "  - **B. Transfer Learning** with **ResNet‑18** or **EfficientNet‑B0** (recommended)\n",
    "- Mixed precision (AMP), early stopping, best‑model checkpoint, and evaluation (confusion matrix + per‑class metrics).\n",
    "- TensorBoard logging.\n",
    "\n",
    "> **How to use**: Upload your images (folder tree) and `dataset.csv` to Google Drive, set the two variables in the **Configuration** cell, then run cells top‑to‑bottom.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7022ebbe",
   "metadata": {},
   "source": [
    "## 0) Runtime & installs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9912bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title (Colab) Check GPU & install deps\n",
    "!nvidia-smi -L || true\n",
    "\n",
    "# Optional: upgrade pip & core libs\n",
    "%pip -q install --upgrade pip\n",
    "%pip -q install pandas scikit-learn matplotlib seaborn tqdm tensorboard\n",
    "\n",
    "# Torch/torchvision come preinstalled on Colab; if needed, uncomment next:\n",
    "# %pip -q install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121\n",
    "\n",
    "import torch, torchvision\n",
    "print('Torch:', torch.__version__, '| Torchvision:', torchvision.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f00a070",
   "metadata": {},
   "source": [
    "## 1) Imports & global config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2dbe27b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, time, math, shutil, random, json\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torch.cuda import amp\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchvision import transforms, models\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from collections import Counter\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Reproducibility\n",
    "def set_seed(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = False  # faster + okay with AMP\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "set_seed(42)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6961f8d8",
   "metadata": {},
   "source": [
    "## 2) Mount Drive (optional) & Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef3e175",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Mount Drive (optional)\n",
    "# If your data is in Google Drive, mount it:\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175792e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#@title Configuration (edit these two paths)\n",
    "# Path to your images ROOT and to the CSV with image paths and labels.\n",
    "IMAGES_ROOT = \"/content/drive/MyDrive/datasets/chest_xray\"  #@param {type:\"string\"}\n",
    "CSV_PATH    = \"/content/drive/MyDrive/datasets/chest_xray/dataset.csv\"  #@param {type:\"string\"}\n",
    "\n",
    "# Column names in CSV (auto-detected if blank)\n",
    "LABEL_COL   = \"\"  # e.g., \"label\" (leave empty to auto-detect)\n",
    "PATH_COL    = \"\"  # e.g., \"img_path\" or \"path\" (leave empty to auto-detect)\n",
    "\n",
    "# Model selection: \"cnn_small\", \"resnet18\", or \"efficientnet_b0\"\n",
    "MODEL_NAME  = \"resnet18\"  #@param [\"cnn_small\", \"resnet18\", \"efficientnet_b0\"]\n",
    "IMAGE_SIZE  = 224         #@param {type:\"integer\"}\n",
    "BATCH_SIZE  = 32          #@param {type:\"integer\"}\n",
    "EPOCHS      = 15          #@param {type:\"integer\"}\n",
    "LR          = 1e-4        #@param {type:\"number\"}\n",
    "WEIGHT_DECAY= 1e-4        #@param {type:\"number\"}\n",
    "\n",
    "# Whether to use WeightedRandomSampler to handle class imbalance\n",
    "USE_WEIGHTED_SAMPLER = True  #@param {type:\"boolean\"}\n",
    "\n",
    "# Where to save checkpoints and logs\n",
    "OUT_DIR = Path('/content/xray_runs')\n",
    "OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print('Outputs:', OUT_DIR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff5e153",
   "metadata": {},
   "source": [
    "## 3) Load CSV & resolve image paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea9e62f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper: normalize Windows/relative paths and find images within IMAGES_ROOT\n",
    "def index_all_images(root: Path):\n",
    "    \"\"\"Create a dict: basename -> [full_paths...] for quick fallback lookups.\"\"\"\n",
    "    idx = {}\n",
    "    for dirpath, _, filenames in os.walk(root):\n",
    "        for fn in filenames:\n",
    "            if fn.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp', '.tiff')):\n",
    "                full = str(Path(dirpath) / fn)\n",
    "                idx.setdefault(fn.lower(), []).append(full)\n",
    "    return idx\n",
    "\n",
    "def normalize_path(s: str) -> str:\n",
    "    # Replace backslashes, strip quotes/spaces\n",
    "    s = str(s).strip().strip('\\\"\\' )\n",
    "    s = s.replace('\\\\', '/').replace('\\\\', '/').replace('\\\\', '/')\n",
    "    s = s.replace('\\\\', '/').replace('\\\\', '/')\n",
    "    s = s.replace('\\\\', '/')\n",
    "    s = s.replace('\\\\', '/')\n",
    "    s = s.replace('\\\\', '/')\n",
    "    s = s.replace('\\', '/')\n",
    "    return s\n",
    "\n",
    "def resolve_paths(df: pd.DataFrame, images_root: Path, path_col: str):\n",
    "    images_root = Path(images_root)\n",
    "    assert images_root.exists(), f\"IMAGES_ROOT not found: {images_root}\"\n",
    "\n",
    "    # Build index once (can be ~1-2s for ~20k images)\n",
    "    print('Indexing images under', images_root, '...')\n",
    "    name_index = index_all_images(images_root)\n",
    "    resolved, missing = [], []\n",
    "    for p in tqdm(df[path_col].tolist(), desc='Resolving paths'):\n",
    "        p_norm = normalize_path(p)\n",
    "        cand = Path(p_norm)\n",
    "        # Case 1: absolute path that actually exists (rare in Colab)\n",
    "        if cand.exists():\n",
    "            resolved.append(str(cand))\n",
    "            continue\n",
    "        # Case 2: treat as relative to IMAGES_ROOT\n",
    "        cand2 = images_root / p_norm\n",
    "        if cand2.exists():\n",
    "            resolved.append(str(cand2))\n",
    "            continue\n",
    "        # Case 3: try basename lookup\n",
    "        base = os.path.basename(p_norm).lower()\n",
    "        hits = name_index.get(base, [])\n",
    "        if len(hits) >= 1:\n",
    "            # If multiple hits, pick the first; consider disambiguation if needed\n",
    "            resolved.append(hits[0])\n",
    "        else:\n",
    "            resolved.append(None)\n",
    "            missing.append(p_norm)\n",
    "    if missing:\n",
    "        print(f\"WARNING: {len(missing)} images not found. First 10:\\n\", missing[:10])\n",
    "    return resolved\n",
    "\n",
    "# Load CSV\n",
    "df = pd.read_csv(CSV_PATH)\n",
    "print('CSV shape:', df.shape)\n",
    "print('Columns:', df.columns.tolist())\n",
    "\n",
    "# Auto-detect columns if user left them blank\n",
    "if not PATH_COL:\n",
    "    candidates = [c for c in df.columns if c.lower() in ['img_path','path','image_path','filepath','file','filename']]\n",
    "    assert len(candidates) >= 1, \"Could not auto-detect the image path column. Set PATH_COL manually.\"\n",
    "    PATH_COL = candidates[0]\n",
    "if not LABEL_COL:\n",
    "    candidates = [c for c in df.columns if c.lower() in ['label','class','target','category']]\n",
    "    assert len(candidates) >= 1, \"Could not auto-detect the label column. Set LABEL_COL manually.\"\n",
    "    LABEL_COL = candidates[0]\n",
    "\n",
    "print('Using PATH_COL =', PATH_COL, '| LABEL_COL =', LABEL_COL)\n",
    "\n",
    "# Resolve full paths\n",
    "df['resolved_path'] = resolve_paths(df, Path(IMAGES_ROOT), PATH_COL)\n",
    "\n",
    "# Drop rows with missing images\n",
    "before = len(df)\n",
    "df = df.dropna(subset=['resolved_path']).reset_index(drop=True)\n",
    "after = len(df)\n",
    "print(f'Retained {after}/{before} rows after resolving image paths.')\n",
    "\n",
    "# Normalize labels (strip spaces)\n",
    "df[LABEL_COL] = df[LABEL_COL].astype(str).str.strip()\n",
    "print(df[LABEL_COL].value_counts())\n",
    "\n",
    "# Look at a few rows\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "408242e3",
   "metadata": {},
   "source": [
    "## 4) Train/Val/Test split & Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3948400",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Classes\n",
    "classes = sorted(df[LABEL_COL].unique().tolist())\n",
    "print('Classes:', classes)\n",
    "cls_to_idx = {c:i for i,c in enumerate(classes)}\n",
    "idx_to_cls = {i:c for c,i in cls_to_idx.items()}\n",
    "\n",
    "# Stratified split 70/15/15\n",
    "df_train, df_temp = train_test_split(df, test_size=0.30, stratify=df[LABEL_COL], random_state=42)\n",
    "df_val, df_test = train_test_split(df_temp, test_size=0.50, stratify=df_temp[LABEL_COL], random_state=42)\n",
    "\n",
    "print('Train:', df_train.shape, 'Val:', df_val.shape, 'Test:', df_test.shape)\n",
    "\n",
    "# Transforms\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std  = [0.229, 0.224, 0.225]\n",
    "\n",
    "train_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.RandomResizedCrop(IMAGE_SIZE, scale=(0.85, 1.0)),\n",
    "    transforms.RandomRotation(5),\n",
    "    # X-rays: usually avoid horizontal flip unless you confirm it's acceptable for your task\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "val_tfms = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=3),\n",
    "    transforms.Resize(int(IMAGE_SIZE * 1.14)),\n",
    "    transforms.CenterCrop(IMAGE_SIZE),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean, std),\n",
    "])\n",
    "\n",
    "class XRayDataset(Dataset):\n",
    "    def __init__(self, df, path_col, label_col, transforms=None, cls_to_idx=None):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.path_col = path_col\n",
    "        self.label_col = label_col\n",
    "        self.transforms = transforms\n",
    "        self.cls_to_idx = cls_to_idx\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        row = self.df.iloc[i]\n",
    "        p = row['resolved_path']\n",
    "        y = self.cls_to_idx[row[self.label_col]]\n",
    "        with Image.open(p) as img:\n",
    "            img = img.convert('L')  # ensure grayscale\n",
    "        if self.transforms:\n",
    "            img = self.transforms(img)\n",
    "        return img, y\n",
    "\n",
    "train_ds = XRayDataset(df_train, 'resolved_path', LABEL_COL, transforms=train_tfms, cls_to_idx=cls_to_idx)\n",
    "val_ds   = XRayDataset(df_val,   'resolved_path', LABEL_COL, transforms=val_tfms,   cls_to_idx=cls_to_idx)\n",
    "test_ds  = XRayDataset(df_test,  'resolved_path', LABEL_COL, transforms=val_tfms,   cls_to_idx=cls_to_idx)\n",
    "\n",
    "# Weighted sampler for imbalance\n",
    "sampler = None\n",
    "if USE_WEIGHTED_SAMPLER:\n",
    "    counts = Counter(df_train[LABEL_COL].tolist())\n",
    "    class_count = torch.tensor([counts[c] for c in classes], dtype=torch.float)\n",
    "    class_weight = 1.0 / class_count\n",
    "    sample_weights = [class_weight[cls_to_idx[label]].item() for label in df_train[LABEL_COL].tolist()]\n",
    "    sampler = WeightedRandomSampler(sample_weights, num_samples=len(sample_weights), replacement=True)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH_SIZE, shuffle=(sampler is None), sampler=sampler, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds,   batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds,  batch_size=BATCH_SIZE, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n",
    "len(train_ds), len(val_ds), len(test_ds)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c2b01e7",
   "metadata": {},
   "source": [
    "## 5) Define Models (Small CNN / ResNet‑18 / EfficientNet‑B0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f49a5081",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SmallCNN(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(3, 32, kernel_size=3, padding=1), nn.BatchNorm2d(32), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 112\n",
    "\n",
    "            nn.Conv2d(32, 64, kernel_size=3, padding=1), nn.BatchNorm2d(64), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 56\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1), nn.BatchNorm2d(128), nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2),  # 28\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1), nn.BatchNorm2d(256), nn.ReLU(inplace=True),\n",
    "            nn.AdaptiveAvgPool2d((1,1)),\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(256, 128), nn.ReLU(inplace=True),\n",
    "            nn.Dropout(0.3),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        x = self.features(x)\n",
    "        x = self.classifier(x)\n",
    "        return x\n",
    "\n",
    "def create_model(name: str, num_classes: int):\n",
    "    name = name.lower()\n",
    "    if name == 'cnn_small':\n",
    "        return SmallCNN(num_classes)\n",
    "    elif name == 'resnet18':\n",
    "        weights = models.ResNet18_Weights.DEFAULT\n",
    "        model = models.resnet18(weights=weights)\n",
    "        # Freeze first layers optionally (uncomment to fine‑tune last layers only)\n",
    "        # for p in list(model.parameters())[:-5]: p.requires_grad = False\n",
    "        in_features = model.fc.in_features\n",
    "        model.fc = nn.Linear(in_features, num_classes)\n",
    "        return model\n",
    "    elif name == 'efficientnet_b0':\n",
    "        weights = models.EfficientNet_B0_Weights.DEFAULT\n",
    "        model = models.efficientnet_b0(weights=weights)\n",
    "        in_features = model.classifier[-1].in_features\n",
    "        model.classifier[-1] = nn.Linear(in_features, num_classes)\n",
    "        return model\n",
    "    else:\n",
    "        raise ValueError(f\"Unknown model: {name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20bc5b46",
   "metadata": {},
   "source": [
    "## 6) Training & Evaluation Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b198afd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, loader, optimizer, scaler, criterion):\n",
    "    model.train()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(device, non_blocking=True), torch.tensor(y, device=device)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        with amp.autocast():\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "        scaler.scale(loss).backward()\n",
    "        scaler.step(optimizer)\n",
    "        scaler.update()\n",
    "\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "@torch.no_grad()\n",
    "def evaluate(model, loader, criterion):\n",
    "    model.eval()\n",
    "    total_loss, correct, total = 0.0, 0, 0\n",
    "    for x, y in tqdm(loader, leave=False):\n",
    "        x, y = x.to(device, non_blocking=True), torch.tensor(y, device=device)\n",
    "        logits = model(x)\n",
    "        loss = criterion(logits, y)\n",
    "        total_loss += loss.item() * x.size(0)\n",
    "        preds = logits.argmax(1)\n",
    "        correct += (preds == y).sum().item()\n",
    "        total += x.size(0)\n",
    "    return total_loss / total, correct / total\n",
    "\n",
    "def save_checkpoint(state, is_best, out_dir: Path, tag='last'):\n",
    "    out_dir = Path(out_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    path = out_dir / f'checkpoint_{tag}.pt'\n",
    "    torch.save(state, path)\n",
    "    if is_best:\n",
    "        best_path = out_dir / 'best.pt'\n",
    "        shutil.copy2(path, best_path)\n",
    "    return path\n",
    "\n",
    "def plot_confusion(y_true, y_pred, labels, normalize=True):\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=range(len(labels)))\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1, keepdims=True)\n",
    "    plt.figure(figsize=(6,5))\n",
    "    sns.heatmap(cm, annot=True, fmt='.2f' if normalize else 'd',\n",
    "                xticklabels=labels, yticklabels=labels)\n",
    "    plt.xlabel('Predicted'); plt.ylabel('True'); plt.title('Confusion Matrix')\n",
    "    plt.tight_layout(); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22efba40",
   "metadata": {},
   "source": [
    "## 7) Train Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f219d061",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = create_model(MODEL_NAME, num_classes=len(classes)).to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.AdamW(model.parameters(), lr=LR, weight_decay=WEIGHT_DECAY)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=2, verbose=True)\n",
    "scaler = amp.GradScaler()\n",
    "\n",
    "best_acc = 0.0\n",
    "patience, bad = 5, 0\n",
    "\n",
    "history = {'epoch': [], 'train_loss': [], 'train_acc': [], 'val_loss': [], 'val_acc': []}\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    print(f\"Epoch {epoch}/{EPOCHS}\")\n",
    "    train_loss, train_acc = train_one_epoch(model, train_loader, optimizer, scaler, criterion)\n",
    "    val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
    "    scheduler.step(val_acc)\n",
    "\n",
    "    history['epoch'].append(epoch)\n",
    "    history['train_loss'].append(train_loss); history['train_acc'].append(train_acc)\n",
    "    history['val_loss'].append(val_loss);     history['val_acc'].append(val_acc)\n",
    "\n",
    "    print(f\"  train: loss={train_loss:.4f} acc={train_acc:.4f} | val: loss={val_loss:.4f} acc={val_acc:.4f}\")\n",
    "\n",
    "    is_best = val_acc > best_acc\n",
    "    if is_best:\n",
    "        best_acc = val_acc\n",
    "        bad = 0\n",
    "    else:\n",
    "        bad += 1\n",
    "\n",
    "    save_checkpoint({\n",
    "        'epoch': epoch,\n",
    "        'model_state_dict': model.state_dict(),\n",
    "        'optimizer_state_dict': optimizer.state_dict(),\n",
    "        'val_acc': val_acc,\n",
    "        'classes': classes,\n",
    "        'model_name': MODEL_NAME,\n",
    "        'image_size': IMAGE_SIZE\n",
    "    }, is_best=is_best, out_dir=OUT_DIR, tag=f'epoch{epoch:03d}')\n",
    "\n",
    "    if bad >= patience:\n",
    "        print(f\"Early stopping (no val acc improvement in {patience} epochs). Best val acc: {best_acc:.4f}\")\n",
    "        break\n",
    "\n",
    "# Save training history\n",
    "with open(OUT_DIR / 'history.json', 'w') as f:\n",
    "    json.dump(history, f, indent=2)\n",
    "\n",
    "print('Best val acc:', best_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89a58046",
   "metadata": {},
   "source": [
    "## 8) Evaluate on Test Set (confusion matrix & per‑class metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fa4720f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "ckpt = torch.load(OUT_DIR / 'best.pt', map_location=device)\n",
    "model = create_model(ckpt.get('model_name', MODEL_NAME), num_classes=len(classes)).to(device)\n",
    "model.load_state_dict(ckpt['model_state_dict'])\n",
    "model.eval()\n",
    "\n",
    "y_true, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(test_loader):\n",
    "        x = x.to(device, non_blocking=True)\n",
    "        logits = model(x)\n",
    "        preds = logits.argmax(1).cpu().numpy().tolist()\n",
    "        y_pred.extend(preds)\n",
    "        y_true.extend(y)\n",
    "\n",
    "print(classification_report(y_true, y_pred, target_names=classes, digits=4))\n",
    "plot_confusion(y_true, y_pred, labels=classes, normalize=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c614f0bb",
   "metadata": {},
   "source": [
    "## 9) Inference on a single image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf39029b",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def predict_image(image_path: str):\n",
    "    model.eval()\n",
    "    img = Image.open(image_path).convert('L')\n",
    "    tfm = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=3),\n",
    "        transforms.Resize(int(IMAGE_SIZE * 1.14)),\n",
    "        transforms.CenterCrop(IMAGE_SIZE),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\n",
    "    ])\n",
    "    x = tfm(img).unsqueeze(0).to(device)\n",
    "    logits = model(x)\n",
    "    probs = torch.softmax(logits, dim=1).squeeze(0).cpu().numpy()\n",
    "    pred_idx = int(np.argmax(probs))\n",
    "    return classes[pred_idx], {cls: float(probs[i]) for i, cls in enumerate(classes)}\n",
    "\n",
    "# Example:\n",
    "# img_path = test_ds.df.sample(1).iloc[0]['resolved_path']\n",
    "# pred, probs = predict_image(img_path)\n",
    "# print('Pred:', pred); probs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e61599",
   "metadata": {},
   "source": [
    "## 10) (Optional) TensorBoard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f2ea6d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# You can also log to TensorBoard if you add SummaryWriter in the train loop.\n",
    "# For now, just show how to launch it (if logs exist):\n",
    "# %load_ext tensorboard\n",
    "# %tensorboard --logdir /content/xray_runs"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
